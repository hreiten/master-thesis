{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble stacking\n",
    "Combining models for better generalizing abilities. Will combine the predictions of the best LSTM model and the MLP model. The performance of the two show that they both do generally good, but that one model predicts better at one target than the other. It is therefore assumed that one model manages to extract some information that the other cannot in predicting specific targets. Overall generalizing abilities of the models can most likely be boosted by a technique called \"ensemble stacking\", where the two models predictions are combined into one final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ROOT_PATH = os.path.abspath(\".\").split(\"src\")[0]\n",
    "module_path = os.path.abspath(os.path.join(ROOT_PATH+\"/src/utils/\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc \n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import functions as f\n",
    "import dl_functions as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure matplotlib params and plotting\n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "sns.set_style('whitegrid', {'axes.grid': True, 'grid.linestyle': '--'})\n",
    "rc('figure', figsize=(12,6))\n",
    "rc('xtick', labelsize=12)\n",
    "rc('ytick', labelsize=12)\n",
    "rc('axes', labelsize=13, titlesize=14)\n",
    "rc('legend', fontsize=14, handlelength=2)\n",
    "rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and metadata\n",
    "df_train, df_valid, df_test = f.load_data()\n",
    "stats, ts, ts_train, ts_valid, ts_test = f.load_metadata()\n",
    "\n",
    "# split datasets into features and targets\n",
    "x_train, y_train = f.split_dataset(df_train.values, delay=1)\n",
    "x_valid, y_valid = f.split_dataset(df_valid.values, delay=1)\n",
    "x_test, y_test = f.split_dataset(df_test.values, delay=1)\n",
    "\n",
    "# metadata\n",
    "target_tags = df_train.columns.values[:3]\n",
    "feature_tags = df_train.columns.values[3:]\n",
    "target_stds = stats.loc[target_tags,\"Std\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_error(preds, targets, target_stdevs=target_stds):\n",
    "    \"\"\"\n",
    "    Will evaluate the MAE of a set of predictions and targets.\n",
    "    \n",
    "    :param preds: Matrix of predictions with shape (n_obs, n_target_variables)\n",
    "    :param targets: Matrix of true targets with shape (n_obs, n_target_variables)\n",
    "    :param target_stdevs: 1D vector of the standard deviations of the target variables.\n",
    "    \n",
    "    :return return_dict: A dictionary with the computed error variables. \n",
    "    \"\"\"\n",
    "    \n",
    "    maes = f.MAE(targets, preds, vector=True)\n",
    "    maes_unstd = (maes * target_stdevs)\n",
    "\n",
    "    err_df = pd.DataFrame(np.column_stack([[\"FT\", \"TT\", \"PT\"], maes, maes_unstd]), \n",
    "                          columns=['Tag', 'MAE (std)', 'MAE (unstd)'])\n",
    "    err_df.loc[\"Avg\"] = err_df.mean()\n",
    "    \n",
    "    str_table = tabulate(err_df, headers='keys', tablefmt='psql', floatfmt='.5f')\n",
    "\n",
    "    return_dict = {\n",
    "        'err_df': err_df,\n",
    "        'err_table': str_table,\n",
    "        'maes': maes,\n",
    "        'maes_unstd': maes_unstd\n",
    "    }\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(lstm_model, mlp_model, X, y, n_pred=30):\n",
    "    lstm_preds = dlf.predict_with_model(lstm_model, X, y, n_predictions=n_pred)[0]\n",
    "    mlp_preds = mlp_model.predict(X)\n",
    "    \n",
    "    Xtr = np.concatenate((lstm_preds, mlp_preds), axis=1)\n",
    "    \n",
    "    # find weights between the predictions of the LSTM model and the MLP by Linear Regr.\n",
    "    lm = LinearRegression().fit(Xtr,y)\n",
    "    return lm\n",
    "\n",
    "def predict(lm, lstm_model, mlp_model, X, y, n_pred=30):\n",
    "    \n",
    "    lstm_preds = dlf.predict_with_model(lstm_model, X, y, n_predictions=n_pred)[0]\n",
    "    mlp_preds = mlp_model.predict(X)\n",
    "    \n",
    "    Xtr = np.concatenate((lstm_preds, mlp_preds), axis=1)\n",
    "    \n",
    "    return lm.predict(Xtr)\n",
    "\n",
    "def evaluate_ensemble(lstm_model, mlp_model, train_tuple, val_tuple, test_tuple, n_pred=30):\n",
    "    lm = train_lm(lstm_model, mlp_model, train_tuple[0], train_tuple[1], n_pred)\n",
    "    \n",
    "    val_preds = predict(lm, lstm_model, mlp_model, val_tuple[0], val_tuple[1], n_pred)\n",
    "    val_errs = evaluate_error(val_tuple[1], val_preds)\n",
    "    \n",
    "    test_preds = predict(lm, lstm_model, mlp_model, test_tuple[0], test_tuple[1], n_pred)\n",
    "    test_errs = evaluate_error(test_tuple[1], test_preds)\n",
    "    \n",
    "    print(\"\\n **** VALIDATION **** \")\n",
    "    print(val_errs['err_table'])\n",
    "    \n",
    "    print(\"\\n **** TEST **** \")\n",
    "    print(test_errs['err_table'])\n",
    "    \n",
    "    return (val_preds, val_errs), (test_preds, test_errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load models\n",
    "lstm_model = dlf.load_keras_model(ROOT_PATH + \"models/lstm_128/50/\")\n",
    "mlp_model = dlf.load_keras_model(ROOT_PATH + \"models/mlp_512/50/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **** VALIDATION **** \n",
      "+-----+-------+-------------+---------------+\n",
      "|     | Tag   |   MAE (std) |   MAE (unstd) |\n",
      "|-----+-------+-------------+---------------|\n",
      "| 0   | FT    |     0.58224 |    1966.05145 |\n",
      "| 1   | TT    |     0.30202 |       0.29493 |\n",
      "| 2   | PT    |     0.40426 |       0.10005 |\n",
      "| Avg | nan   |   nan       |     nan       |\n",
      "+-----+-------+-------------+---------------+\n",
      "\n",
      " **** TEST **** \n",
      "+-----+-------+-------------+---------------+\n",
      "|     | Tag   |   MAE (std) |   MAE (unstd) |\n",
      "|-----+-------+-------------+---------------|\n",
      "| 0   | FT    |     0.51240 |    1730.20578 |\n",
      "| 1   | TT    |     0.24929 |       0.24344 |\n",
      "| 2   | PT    |     0.38778 |       0.09597 |\n",
      "| Avg | nan   |   nan       |     nan       |\n",
      "+-----+-------+-------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "n_pred = 30\n",
    "val_ensemble, test_ensemble = evaluate_ensemble(lstm_model, \n",
    "                                                mlp_model, \n",
    "                                                (x_train, y_train), \n",
    "                                                (x_valid, y_valid), \n",
    "                                                (x_test, y_test), \n",
    "                                                n_pred=n_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame(np.vstack(val_data), index = indexes, columns=columns)\n",
    "test_df = pd.DataFrame(np.vstack(test_data), index = indexes, columns=columns)\n",
    "summary_df = pd.concat([val_df, test_df], axis=1, keys=[\"Validation\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(val_dicts, test_dicts, columns, indexes, unstd=False, texpath=None, round_digits=4):\n",
    "    \"\"\"\n",
    "    Method that concatenates validation and test dictionaries (as obtained by evaluate_error()) into a dataframe.\n",
    "    Is typically used to get a summary of multiple methods and their performance on test and validation data. \n",
    "    \n",
    "    :param val_dicts: Validation dictionaries as obtained by evalueate_error()\n",
    "    :param test_dicts: Test dictionaries as obtained by evaluate_error()\n",
    "    :param columns: Vector of column names for the dataframe\n",
    "    :param indexes: Vector of index names for the dataframe\n",
    "    :param unstd: Boolean if the MAEs should be unstandardized or not. Default=False. \n",
    "    :param texpath: Path to save the obtained latex output. Default=None. \n",
    "    :round_digit: The significant digits to round a decimal to, used in format_digit(). Default=4. \n",
    "    \n",
    "    :return: The summary dataframe and the latex string of the dataframe. \n",
    "    \"\"\"\n",
    "    \n",
    "    name = 'maes_unstd' if unstd else 'maes'\n",
    "    val_data = []\n",
    "    for i, d in enumerate(val_dicts):\n",
    "        tmp = np.append(d[name], d['avg_mae'])\n",
    "        tmp = [format_digit(digit) for digit in tmp]\n",
    "        val_data.append(tmp)\n",
    "\n",
    "    test_data = []\n",
    "    for i, d in enumerate(test_dicts):\n",
    "        tmp = np.append(d[name], d['avg_mae'])\n",
    "        tmp = [format_digit(digit) for digit in tmp]\n",
    "        test_data.append(tmp)\n",
    "\n",
    "    val_df = pd.DataFrame(np.vstack(val_data), index = indexes, columns=columns)\n",
    "    test_df = pd.DataFrame(np.vstack(test_data), index = indexes, columns=columns)\n",
    "    summary_df = pd.concat([val_df, test_df], axis=1, keys=[\"Validation\", \"Test\"])\n",
    "    \n",
    "    tex = latexify(summary_df)\n",
    "\n",
    "    if texpath is not None: # save the file\n",
    "        with open(texpath, 'w+') as f:\n",
    "            f.write(tex)\n",
    "    \n",
    "    return summary_df, tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tag</th>\n",
       "      <th>MAE (Standardized)</th>\n",
       "      <th>MAE (Unstandardized)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FT</td>\n",
       "      <td>0.5829793440798184</td>\n",
       "      <td>1968.5339519571417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TT</td>\n",
       "      <td>0.2794662369312663</td>\n",
       "      <td>0.27290628450898524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PT</td>\n",
       "      <td>0.4055721406955908</td>\n",
       "      <td>0.10037388738909701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avg</td>\n",
       "      <td>0.422673</td>\n",
       "      <td>656.302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tag  MAE (Standardized) MAE (Unstandardized)\n",
       "0   FT  0.5829793440798184   1968.5339519571417\n",
       "1   TT  0.2794662369312663  0.27290628450898524\n",
       "2   PT  0.4055721406955908  0.10037388738909701\n",
       "3  Avg            0.422673              656.302"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ensemble[1]['err_df']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

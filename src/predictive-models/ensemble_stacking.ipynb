{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble stacking\n",
    "Combining models for better generalizing abilities. Will combine the predictions of the best LSTM model and the MLP model. The performance of the two show that they both do generally good, but that one model predicts better at one target than the other. It is therefore assumed that one model manages to extract some information that the other cannot in predicting specific targets. Overall generalizing abilities of the models can most likely be boosted by a technique called \"ensemble stacking\", where the two models predictions are combined into one final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ROOT_PATH = os.path.abspath(\".\").split(\"src\")[0]\n",
    "module_path = os.path.abspath(os.path.join(ROOT_PATH+\"/src/utils/\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rc \n",
    "from tabulate import tabulate\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import functions as f\n",
    "import dl_functions as dlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure matplotlib params and plotting\n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "sns.set_style('whitegrid', {'axes.grid': True, 'grid.linestyle': '--'})\n",
    "rc('figure', figsize=(12,6))\n",
    "rc('xtick', labelsize=12)\n",
    "rc('ytick', labelsize=12)\n",
    "rc('axes', labelsize=13, titlesize=14)\n",
    "rc('legend', fontsize=14, handlelength=2)\n",
    "rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and metadata\n",
    "df_train, df_valid, df_test = f.load_data()\n",
    "stats, ts, ts_train, ts_valid, ts_test = f.load_metadata()\n",
    "\n",
    "# split datasets into features and targets\n",
    "x_train, y_train = f.split_dataset(df_train.values, delay=1)\n",
    "x_valid, y_valid = f.split_dataset(df_valid.values, delay=1)\n",
    "x_test, y_test = f.split_dataset(df_test.values, delay=1)\n",
    "\n",
    "# metadata\n",
    "target_tags = df_train.columns.values[:3]\n",
    "feature_tags = df_train.columns.values[3:]\n",
    "target_stds = stats.loc[target_tags,\"Std\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_error(mean_preds, std_preds, y_true, target_stds=target_stds):\n",
    "    \"\"\"\n",
    "    Will evaluate the MAE of a set of predictions and targets.\n",
    "    \n",
    "    :param preds: Matrix of predictions with shape (n_obs, n_target_variables)\n",
    "    :param targets: Matrix of true targets with shape (n_obs, n_target_variables)\n",
    "    :param target_stdevs: 1D vector of the standard deviations of the target variables.\n",
    "    \n",
    "    :return return_dict: A dictionary with the computed error variables. \n",
    "    \"\"\"\n",
    "    maes = f.MAE(y_true, mean_preds, vector=True)\n",
    "    maes_unstd = maes * target_stds\n",
    "    \n",
    "    expected_mean = np.mean(mean_preds, axis=0)\n",
    "    expected_std = np.mean(std_preds, axis=0)\n",
    "\n",
    "    # summarize in dataframe\n",
    "    indexes = [\"FT\", \"TT\", \"PT\"]\n",
    "    cols = [\"MAE (std)\", \"MAE (unstd)\", \"Expect. Mean\", \"Expect. Stdev\"]\n",
    "    data = np.column_stack([maes, maes_unstd, expected_mean, expected_std])\n",
    "    df = pd.DataFrame(data, index=indexes, columns=cols)\n",
    "    df.loc[\"Avg\"] = df.mean()\n",
    "    \n",
    "    str_table = tabulate(df, headers='keys', tablefmt='psql', floatfmt='.5f')\n",
    "\n",
    "    return_dict = {\n",
    "        'df': df,\n",
    "        'str_table': str_table,\n",
    "        'maes': maes,\n",
    "        'maes_unstd': maes_unstd\n",
    "    }\n",
    "\n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(lstm_model, mlp_model, x_train, y_train, n_pred=30):\n",
    "    lstm_preds = dlf.predict_with_model(lstm_model, x_train, y_train, n_predictions=n_pred)[0]\n",
    "    mlp_preds = dlf.predict_with_model(mlp_model, x_train, y_train, n_predictions=n_pred, input_dim=2)[0]\n",
    "    \n",
    "    Xtr = np.concatenate((lstm_preds, mlp_preds), axis=1)\n",
    "    \n",
    "    # find weights between the predictions of the LSTM model and the MLP by Linear Regr.\n",
    "    lm = LinearRegression().fit(Xtr,y_train)\n",
    "    return lm\n",
    "\n",
    "\n",
    "def predict_lm(lm, lstm_model, mlp_model, X, y, n_pred=30, return_predictions=False):\n",
    "    # predict with lstm\n",
    "    lstm_means, lstm_stds, lstm_dict = dlf.predict_with_model(lstm_model, \n",
    "                                                              X, y, \n",
    "                                                              n_predictions=n_pred, \n",
    "                                                              input_dim=3)\n",
    "    lstm_preds = lstm_dict['pred_matr']\n",
    "\n",
    "    mlp_means, mlp_stds, mlp_dict = dlf.predict_with_model(mlp_model, \n",
    "                                                           X, y, \n",
    "                                                           n_predictions=n_pred, \n",
    "                                                           input_dim=2)\n",
    "    mlp_preds = mlp_dict['pred_matr']\n",
    "\n",
    "    lm_preds_matrix = np.zeros(shape=lstm_preds.shape)\n",
    "    pred_stds = np.zeros(shape=lstm_preds[0].shape)\n",
    "    pred_means = np.zeros(shape=lstm_preds[0].shape)\n",
    "\n",
    "    for t in range(len(X)):\n",
    "        row_x = np.concatenate((lstm_preds[:,t,:], mlp_preds[:,t,:]), axis=1)\n",
    "        pred = lm.predict(row_x)\n",
    "\n",
    "        lm_preds_matrix[:,t,:] = pred\n",
    "        pred_means[t,:] = np.mean(pred, axis=0)\n",
    "        pred_stds[t,:] = np.std(pred, axis=0)\n",
    "    \n",
    "    if return_predictions: \n",
    "        return pred_means, pred_stds, lm_preds_matrix\n",
    "    else:\n",
    "        return pred_means, pred_stds\n",
    "    \n",
    "    \n",
    "def evaluate_ensemble(pred_model, lstm_model, mlp_model, train_tuple, \n",
    "                      val_tuple, test_tuple, n_pred=30):\n",
    "    lm = train_lm(lstm_model, mlp_model, train_tuple[0], train_tuple[1], n_pred)\n",
    "    \n",
    "    val_preds, val_stds = pred_model(lm, lstm_model, mlp_model, val_tuple[0], val_tuple[1], n_pred)\n",
    "    val_errs = evaluate_error(val_preds, val_stds, val_tuple[1])\n",
    "    \n",
    "    print(\"\\n **** VALIDATION **** \")\n",
    "    print(val_errs['str_table'])\n",
    "    \n",
    "    test_preds, test_stds = pred_model(lm, lstm_model, mlp_model, test_tuple[0], test_tuple[1], n_pred)\n",
    "    test_errs = evaluate_error(test_preds, test_stds, test_tuple[1])\n",
    "    \n",
    "    print(\"\\n **** TEST **** \")\n",
    "    print(test_errs['str_table'])\n",
    "    \n",
    "    return val_errs, test_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# load models\n",
    "lstm_model = dlf.load_keras_model(ROOT_PATH + \"models/lstm_128/50/\")\n",
    "mlp_model = dlf.load_keras_model(ROOT_PATH + \"models/mlp_1024/50/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **** VALIDATION **** \n",
      "+-----+-------------+---------------+----------------+-----------------+\n",
      "|     |   MAE (std) |   MAE (unstd) |   Expect. Mean |   Expect. Stdev |\n",
      "|-----+-------------+---------------+----------------+-----------------|\n",
      "| FT  |     0.58246 |    1966.76916 |       -0.26479 |         0.21083 |\n",
      "| TT  |     0.28140 |       0.27479 |        1.29391 |         0.25039 |\n",
      "| PT  |     0.40585 |       0.10044 |        0.01516 |         0.18565 |\n",
      "| Avg |     0.42323 |     655.71480 |        0.34809 |         0.21563 |\n",
      "+-----+-------------+---------------+----------------+-----------------+\n",
      "\n",
      " **** TEST **** \n",
      "+-----+-------------+---------------+----------------+-----------------+\n",
      "|     |   MAE (std) |   MAE (unstd) |   Expect. Mean |   Expect. Stdev |\n",
      "|-----+-------------+---------------+----------------+-----------------|\n",
      "| FT  |     0.51099 |    1725.43436 |       -0.38791 |         0.22157 |\n",
      "| TT  |     0.26899 |       0.26267 |        1.74810 |         0.28109 |\n",
      "| PT  |     0.38559 |       0.09543 |       -0.00125 |         0.20130 |\n",
      "| Avg |     0.38852 |     575.26416 |        0.45298 |         0.23466 |\n",
      "+-----+-------------+---------------+----------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "n_pred = 300\n",
    "val_lm, test_lm = evaluate_ensemble(predict_lm, \n",
    "                                    lstm_model, \n",
    "                                    mlp_model, \n",
    "                                    (x_train, y_train), \n",
    "                                    (x_valid, y_valid), \n",
    "                                    (x_test, y_test), \n",
    "                                    n_pred=n_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lcccccccc}\n",
      "\\toprule\n",
      "{} & \\multicolumn{4}{c}{Validation} & \\multicolumn{4}{c}{Test} \\\\\n",
      "{} &         FT &      TT &      PT &     Avg &     FT &     TT &      PT &     Avg \\\\\n",
      "\\midrule\n",
      "\\textbf{LSTM/MLP} &     0.5825 &  0.2814 &  0.4058 &  0.4232 &  0.511 &  0.269 &  0.3856 &  0.3885 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "         Validation                           Test                       \n",
      "                 FT      TT      PT     Avg     FT     TT      PT     Avg\n",
      "LSTM/MLP     0.5825  0.2814  0.4058  0.4232  0.511  0.269  0.3856  0.3885\n",
      "\n",
      "\\begin{tabular}{llcccccc}\n",
      "\\toprule\n",
      "         &     & \\multicolumn{3}{c}{Validation} & \\multicolumn{3}{c}{Test} \\\\\n",
      "         &     &        MAE & Exp. Mean & Exp. Std &     MAE & Exp. Mean & Exp. Std \\\\\n",
      "\\midrule\n",
      "\\multirow{4}{*}{\\textbf{LSTM/MLP}} & \\textbf{FT} &     0.5825 &   -0.2648 &   0.2108 &  0.5110 &   -0.3879 &   0.2216 \\\\\n",
      "         & \\textbf{TT} &     0.2814 &    1.2939 &   0.2504 &  0.2690 &    1.7481 &   0.2811 \\\\\n",
      "         & \\textbf{PT} &     0.4058 &    0.0152 &   0.1857 &  0.3856 &   -0.0013 &   0.2013 \\\\\n",
      "         & \\textbf{Avg} &     0.4232 &    0.3481 &   0.2156 &  0.3885 &    0.4530 &   0.2347 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "             Validation                       Test                   \n",
      "                    MAE Exp. Mean Exp. Std     MAE Exp. Mean Exp. Std\n",
      "LSTM/MLP FT      0.5825   -0.2648   0.2108  0.5110   -0.3879   0.2216\n",
      "         TT      0.2814    1.2939   0.2504  0.2690    1.7481   0.2811\n",
      "         PT      0.4058    0.0152   0.1857  0.3856   -0.0013   0.2013\n",
      "         Avg     0.4232    0.3481   0.2156  0.3885    0.4530   0.2347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_dict = {\n",
    "    \"validation\": val_lm,\n",
    "    \"test\": test_lm\n",
    "}\n",
    "\n",
    "# Error summary\n",
    "dicts = [ensemble_dict]\n",
    "columns=[\"FT\", \"TT\", \"PT\", \"Avg\"]\n",
    "index = [\"LSTM/MLP\"]\n",
    "summary, tex = dlf.get_df_from_dicts(dicts, columns, index)\n",
    "print(tex)\n",
    "print(summary)\n",
    "print()\n",
    "\n",
    "# Uncertainty summary\n",
    "columns=[\"MAE\", \"Exp. Mean\", \"Exp. Std\"]\n",
    "index = [\"LSTM/MLP\"]\n",
    "levels = [\"FT\",\"TT\",\"PT\",\"Avg\"]\n",
    "ensemble_unc, tex_unc = dlf.get_uncertainty_df_from_dicts(dicts, columns, index, levels)\n",
    "print(tex_unc)\n",
    "print(ensemble_unc)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ROOT_PATH + \"models/dataframes/\"\n",
    "summary.to_pickle(path + \"ensemble_summary_df.pkl\")\n",
    "ensemble_unc.to_pickle(path + \"ensemble_uncertainty_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.save_pickle(lm, ROOT_PATH + \"models/ensemble/linear_model.pkl\")\n",
    "lstm_model.save(ROOT_PATH + \"models/ensemble/lstm_model.h5\")\n",
    "mlp_model.save(ROOT_PATH + \"models/ensemble/mlp_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "lstm = keras.models.load_model(ROOT_PATH + \"models/ensemble/lstm_model.h5\")\n",
    "mlp = keras.models.load_model(ROOT_PATH + \"models/ensemble/mlp_model.h5\")\n",
    "lm = f.load_pickle(ROOT_PATH + \"models/ensemble/linear_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'dl_functions' has no attribute 'predict_with_ensemble'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e319bce5c9e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m mean_preds, std_preds, pred_matr = dlf.predict_with_ensemble(lm, lstm, mlp, \n\u001b[0m\u001b[1;32m      3\u001b[0m                                                              \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                              \u001b[0mn_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                              return_pred_matr=True)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'dl_functions' has no attribute 'predict_with_ensemble'"
     ]
    }
   ],
   "source": [
    "n_pred = 100\n",
    "mean_preds, std_preds, pred_matr = dlf.predict_with_ensemble(lm, lstm, mlp, \n",
    "                                                             x_test, y_test, \n",
    "                                                             n_pred=n_pred, \n",
    "                                                             return_pred_matr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlf.plot_pred_matrix(pred_matr, x_test, y_test, ts_test, target_tags,\n",
    "                    start_idx=600, n_obs=200, plotCI=True, z=1.645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
